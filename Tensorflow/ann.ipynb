{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Tutorial\n",
    "Tensorflow is an open source deep learning library by Google.It is one of the most popular library out there.\n",
    "This tutorial shows a simple neural network applied to one of the most famous dataset of deep learning.MNIST data set will be used.It contains handwritten digits from 0-9.Tensorflow tutorials have mnist dataset preloaded. \n",
    "Tensorflow has an amazing visualization tool known as tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\", one_hot=True) #one hot true does one hot encoding\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch is one round of forward propogation and one round of backpropogation.There are total 10 labels(0-9) and there are a total of 10,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch=10\n",
    "display_epoch = 1\n",
    "logs_path = 'mnist'\n",
    "n_classes = 10  #Total 10 digits\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data are taken as placeholders.These palceholders are feeded with dictionary.For x placeholder it expects input of [somebatchsize,784(28*28)] and y expects as [somebatchsize,10].The reason of giving none is that we can change our batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('inputs') as inputs:\n",
    "        x = tf.placeholder(tf.float32, shape=(None,784), name='x')\n",
    "        y = tf.placeholder(tf.float32, shape=(None,10), name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize weights we use xavier initializer.Xavier initializer chooses values in random from a gaussian distribution based on the input and output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_xavi(shape, xavier_params = (None, None)):\n",
    "    with tf.variable_scope('Weights'):\n",
    "        (fan_in, fan_out) = xavier_params\n",
    "        low = -4*np.sqrt(6.0/(fan_in + fan_out)) # {sigmoid:4, tanh:1} \n",
    "        high = 4*np.sqrt(6.0/(fan_in + fan_out))\n",
    "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural net is a function that takes two inputs as parameters.The first parameter la expects the input layer/previous layer.The second parameter takes dimension of the previous layer and the dimension of the current layer.The function does a matrix multiplication on previous layer with the current layer weights w and then sums with bias b. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neuralnet(la,shape=[None,None]):\n",
    "    with tf.name_scope('neuralnet'):\n",
    "        w=weights_xavi(shape,xavier_params=(shape[0],shape[1]))\n",
    "        b=weights_xavi([shape[1],],xavier_params=(0,shape[1]))\n",
    "        act=tf.add(tf.matmul(la,w),b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network function takes in input as data.That data is feeded into the first neuralnet function with 500 nodes as the first hidden layer.Layer 2 and Layer 3 also has 500 nodes.Relu activation is used after each neuralnet function.Relu activation makes sure which neurons to fire and which shouldn't.The third layer is connected with 10 nodes which is the outplut layer for our digits(0-9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def network(data):\n",
    "    l1=neuralnet(data,shape=[784,500])\n",
    "    l1=tf.nn.relu(l1)                          #Layer1\n",
    "    l2=neuralnet(l1,shape=[500,500])\n",
    "    l2=tf.nn.relu(l2)                          #Layer2\n",
    "    l3=neuralnet(l2,shape=[500,500])\n",
    "    l3=tf.nn.relu(l3)                          #Layer3\n",
    "    l4=neuralnet(l3,shape=[500,10])\n",
    "    return l4    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function used is softmax_cross_entropy and the optimization function used is Adam.Adam optimizer works better than Gradient Descent algorithm.In tensorflow a graph is built and executed using tf.Session(). To execute each function and variable sess.run() is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed out of 10 loss: 1072.03007977\n",
      "Epoch 1 completed out of 10 loss: 170.869018588\n",
      "Epoch 2 completed out of 10 loss: 90.434417318\n",
      "Epoch 3 completed out of 10 loss: 57.4140657142\n",
      "Epoch 4 completed out of 10 loss: 48.98067365\n",
      "Epoch 5 completed out of 10 loss: 49.470284244\n",
      "Epoch 6 completed out of 10 loss: 42.9887020546\n",
      "Epoch 7 completed out of 10 loss: 51.9657474167\n",
      "Epoch 8 completed out of 10 loss: 40.9384374551\n",
      "Epoch 9 completed out of 10 loss: 33.6574335661\n",
      "Accuracy: 0.9674\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('Model'): \n",
    "    pred = network(x)                      #we get a output of our network\n",
    "with tf.name_scope('Loss'):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    tf.summary.scalar(\"Loss\",cost)         #we find the cost using soft cross entopy.\n",
    "with tf.name_scope('ADAM'):\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)  #We use Adam Optimizer\n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))   #Calculate Accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()        \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())       #Initializes All the variables\n",
    "    summary_writer = tf.summary.FileWriter(logs_path)  #WritesTheGraphForTensorboard\n",
    "    summary_writer.add_graph(graph=sess.graph)        \n",
    "    gstep=1\n",
    "    for i in range(epoch):\n",
    "            epoch_loss = 0             #calculate loss\n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size) #Selects input and label\n",
    "                #The optimizer,Cost,Tensorboard Function are Run.\n",
    "                _, c,summary = sess.run([optimizer, cost,merged_summary_op], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "                summary_writer.add_summary(summary,gstep) #Writes Summary\n",
    "                gstep+=1\n",
    "\n",
    "            print('Epoch', i, 'completed out of',epoch,'loss:',epoch_loss)\n",
    "    print('Accuracy:',accuracy.eval({x:mnist.test.images, y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Sure to run the terminal where 'mnist' is saved.    \n",
    "tensorboard --logdir='mnist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the command line:\n",
      "--> tensorboard --logdir='mnist' \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir='mnist' \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tensorboard Graph Visualization\n",
    "<img src=\"images/ann.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy And Loss\n",
    "<img src=\"images/annac.png\">\n",
    "We can see that accuracy increases and the loss decreases.Thus our network is working properly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
