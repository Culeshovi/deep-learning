{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Tutorial\n",
    "Tensorflow is an open source deep learning library by Google.It is one of the most popular library out there.\n",
    "This tutorial shows a convolution neural network applied to one of the most famous dataset of deep learning.MNIST data set will be used.It contains handwritten digits from 0-9.Tensorflow tutorials have mnist dataset preloaded.Recurrent neural network are widely used in natural language processing,chatbots,language translatiom,etc.\n",
    "Tensorflow has an amazing visualization tool known as tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist=input_data.read_data_sets(\"MNIST_data/\", one_hot=True) #one hot true does one hot encoding\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch is one round of forward propogation and one round of backpropogation.There are total 10 labels(0-9) and there are a total of 10,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch=10\n",
    "display_epoch = 1\n",
    "logs_path = 'mnist'\n",
    "n_classes = 10  #Total 10 digits\n",
    "hm_epochs = 3\n",
    "batch_size = 100\n",
    "n_input = 28\n",
    "timesteps = 28\n",
    "rnn_size = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data are taken as placeholders.These palceholders are feeded with dictionary.For x placeholder it expects input of [somebatchsize,28,28] and y expects as [somebatchsize,10].The reason of giving none is that we can change our batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('inputs') as inputs:\n",
    "        x = tf.placeholder(tf.float32, shape=(None,timesteps,n_input), name='x')\n",
    "        y = tf.placeholder(tf.float32, shape=(None,10), name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To initialize weights we use xavier initializer.Xavier initializer chooses values in random from a gaussian distribution based on the input and output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_xavi(shape, xavier_params = (None, None)):\n",
    "    with tf.variable_scope('Weights'):\n",
    "        (fan_in, fan_out) = xavier_params\n",
    "        low = -4*np.sqrt(6.0/(fan_in + fan_out)) # {sigmoid:4, tanh:1} \n",
    "        high = 4*np.sqrt(6.0/(fan_in + fan_out))\n",
    "        return tf.Variable(tf.random_uniform(shape, minval=low, maxval=high, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural net is a function that takes two inputs as parameters.The first parameter la expects the input layer/previous layer.The second parameter takes dimension of the previous layer and the dimension of the current layer.The function does a matrix multiplication on previous layer with the current layer weights w and then sums with bias b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neuralnet(la,shape=[None,None]):\n",
    "    with tf.name_scope('neuralnet'):\n",
    "        w=weights_xavi(shape,xavier_params=(shape[0],shape[1]))\n",
    "        b=weights_xavi([shape[1],],xavier_params=(0,shape[1]))\n",
    "        act=tf.add(tf.matmul(la,w),b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural net is a function that takes two inputs as parameters.The first parameter la expects the input image/previous layer.The second parameter takes dimension as [kernel size,kernel size,previous no of channels,current no of channels].The function does convolution operation on previous layer with the current layer weights w and then sums with bias b. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNN cell takes in input x and gives two outputs. The state outputs become inputs along with x in the next time step. This state outputs help in preserving the memory of the cell. However during backpropogation RNN face a problem of vanishing gradient. To solve this problem LSTM cell are used. To know more about LSTM visit the blog :- http://colah.github.io/posts/2015-08-Understanding-LSTMs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_cell():\n",
    "    return tf.contrib.rnn.BasicRNNCell(rnn_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lstm_cell():\n",
    "    return tf.contrib.rnn.BasicLSTMCell(rnn_size,forget_bias=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our model the inputs are unstacked according to timesteps and are fed to the LSTM cell. The output from LSTM is then multiplied with weights and biases to give the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def RNN(la,shape=[None,None]):\n",
    "    with tf.variable_scope('RNN'):\n",
    "        la =tf.unstack(la, timesteps, 1)\n",
    "        w=weights_xavi(shape,xavier_params=(shape[-2],shape[-1]))\n",
    "        b=weights_xavi([shape[-1],],xavier_params=(0,shape[-1]))\n",
    "        lscell=lstm_cell()\n",
    "        output,states=tf.contrib.rnn.static_rnn(lscell,la,dtype=tf.float32)\n",
    "        act=tf.add(tf.matmul(output[-1],w),b)\n",
    "        tf.summary.histogram(\"weights\", w)\n",
    "        tf.summary.histogram(\"biases\", b)\n",
    "        tf.summary.histogram(\"activations\", act)\n",
    "        return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function used is softmax_cross_entropy and the optimization function used is Adam.Adam optimizer works better than Gradient Descent algorithm.In tensorflow a graph is built and executed using tf.Session(). To execute each function and variable sess.run() is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed out of 10 loss: 207.914453328\n",
      "Epoch 1 completed out of 10 loss: 56.6717027565\n",
      "Epoch 2 completed out of 10 loss: 38.6725350488\n",
      "Epoch 3 completed out of 10 loss: 29.4090174411\n",
      "Epoch 4 completed out of 10 loss: 24.812942869\n",
      "Epoch 5 completed out of 10 loss: 18.9755650715\n",
      "Epoch 6 completed out of 10 loss: 17.1648511103\n",
      "Epoch 7 completed out of 10 loss: 14.9044732371\n",
      "Epoch 8 completed out of 10 loss: 13.2384526202\n",
      "Epoch 9 completed out of 10 loss: 11.3864813036\n",
      "Accuracy: 0.9875\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('Model'): \n",
    "    pred = RNN(x,shape=[500,10])                      #we get a output of our network\n",
    "with tf.name_scope('Loss'):\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "    tf.summary.scalar(\"Loss\",cost)         #we find the cost using soft cross entopy.\n",
    "with tf.name_scope('ADAM'):\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)  #We use Adam Optimizer\n",
    "with tf.name_scope('Accuracy'):\n",
    "    correct = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))   #Calculate Accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "# Merge all summaries into a single op\n",
    "merged_summary_op = tf.summary.merge_all()        \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())       #Initializes All the variables\n",
    "    summary_writer = tf.summary.FileWriter(logs_path)  #WritesTheGraphForTensorboard\n",
    "    summary_writer.add_graph(graph=sess.graph)        \n",
    "    gstep=1\n",
    "    for i in range(epoch):\n",
    "            epoch_loss = 0             #calculate loss\n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                epoch_x=epoch_x.reshape((batch_size,timesteps,n_input))#Selects input and label\n",
    "                #The optimizer,Cost,Tensorboard Function are Run.\n",
    "                _, c,summary = sess.run([optimizer, cost,merged_summary_op], feed_dict={x: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "                summary_writer.add_summary(summary,gstep) #Writes Summary\n",
    "                gstep+=1\n",
    "\n",
    "            print('Epoch', i, 'completed out of',epoch,'loss:',epoch_loss)\n",
    "    print('Accuracy:',accuracy.eval({x:mnist.test.images.reshape((-1,timesteps,n_input)), y:mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the command line:\n",
      "--> tensorboard --logdir='mnist' \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir='mnist' \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tensorboard Graph Visualization\n",
    "<img src ='images/rnn.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy And Losses\n",
    "<img src='images/arn.png'>\n",
    "We can see that accuracy increases and the loss decreases.Thus our network is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
