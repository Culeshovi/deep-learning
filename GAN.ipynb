{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversial Networks(GANs)\n",
    "\n",
    "They were introduced by Ian Goodfellow in his research paper in 2014. According to Facebook's AI research director Yann LeCun GANs are the most interesting idea in the last 10 years in ML. It consits of two networks a discriminator and a generator. A generator is a network that tries to create fake data identical to real data and discriminator tries to distinguish between fake and real data. To give an example let's assume Genrator creates counterfeit money and it's the job of the Discriminator to distinguish between real money and counterfeit money. Over time the Generator becomes better in creating counterfeit money which is identical to real money. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as pkl  #Importing Modules\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os,imageio,itertools,time,pickle\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist=input_data.read_data_sets(\"MNIST_data/\", one_hot=True,reshape=[]) #one hot true does one hot encoding\n",
    "tf.reset_default_graph()\n",
    "batch_size=100\n",
    "lr=0.0002\n",
    "epochs=100\n",
    "display_epoch = 1\n",
    "logs_path = 'vgan'\n",
    "smooth = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "The discriminator takes a 28X28 matrix and tries to predict wether it is fake or real. It contains only one hidden layer of 128 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x,reuse=False):\n",
    "    with tf.variable_scope('discriminator', reuse=reuse):\n",
    "        #layer1\n",
    "        l1=tf.layers.dense(x, 128, activation=None)\n",
    "        l1=tf.nn.leaky_relu(l1,alpha=0.01)\n",
    "        #layer2\n",
    "        l2=tf.layers.dense(l1, 1, activation=None)\n",
    "        o=tf.nn.sigmoid(l2)\n",
    "        return o,l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "The generator tries to predict the real data distribution. It takes a random uniform distribution as input and learns to map it to the real data. It also contains one hidden layer of 128 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z_dim,reuse=False):\n",
    "    with tf.variable_scope('generator', reuse=reuse):\n",
    "        #layer1\n",
    "        l1=tf.layers.dense(z_dim, 128, activation=None)\n",
    "        l1=tf.nn.leaky_relu(l1,alpha=0.01)\n",
    "        #layer2\n",
    "        l2=tf.layers.dense(l1, 784, activation=None)\n",
    "        l2=tf.nn.tanh(l2)\n",
    "        return l2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "The following few lines of code help in visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = 'MNIST_DCGAN_results/'\n",
    "model = 'MNIST_DCGAN_'\n",
    "if not os.path.isdir(root):\n",
    "    os.mkdir(root)\n",
    "if not os.path.isdir(root + 'Fixed_results'):\n",
    "    os.mkdir(root + 'Fixed_results')\n",
    "\n",
    "fixed_z_ = np.random.uniform(-1, 1, size=(batch_size, 100))\n",
    "def show_result(num_epoch, show = False, save = False, path = 'result.png'):\n",
    "\n",
    "    test_images = sess.run(g_i, {z_dim: fixed_z_,})\n",
    "    test_images=np.reshape(test_images,[-1,28,28,1])\n",
    "    size_figure_grid = 5\n",
    "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n",
    "    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
    "        ax[i, j].get_xaxis().set_visible(False)\n",
    "        ax[i, j].get_yaxis().set_visible(False)\n",
    "    for k in range(size_figure_grid*size_figure_grid):\n",
    "        i = k // size_figure_grid\n",
    "        j = k % size_figure_grid\n",
    "        ax[i, j].cla()\n",
    "        ax[i, j].imshow(np.reshape(test_images[k], (28, 28)), cmap='gray')\n",
    "    label = 'Epoch {0}'.format(num_epoch)\n",
    "    fig.text(0.5, 0.04, label, ha='center')\n",
    "\n",
    "    if save:\n",
    "        plt.savefig(path)\n",
    "\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('inputs') as inputs: #Inputs Of Our Network\n",
    "        x = tf.placeholder(tf.float32, shape=(None,784), name='x')\n",
    "        z_dim = tf.placeholder(tf.float32, shape=(None,100), name='z_dim') #z_dim is for our random distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = generator(z_dim,reuse=False) #Generator takes the input z_dim and returns a 28*28(784) matrix.\n",
    "g_i = generator(z_dim,reuse=True) #Reuse of variables is True as g_i will be used for visualization.\n",
    "gs=tf.summary.image(\"generator_i\",tf.reshape(g,shape=(-1,28,28,1)), max_outputs=10) #Tensorboard Image Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d_t ,d_tlog= discriminator(x,reuse=False) #Real Data as Input\n",
    "d_f ,d_flog= discriminator(g,reuse=True) #Fake Data as Input and Reuse of Variables as True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Discriminator_Loss_Fake'): #The Discriminator will set Generator's Fake Data as 0\n",
    "    dfcost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_flog, labels=tf.zeros_like(d_f)))\n",
    "    dfcosts=tf.summary.scalar(\"Discriminator_Loss_Fake\",dfcost)\n",
    "\n",
    "with tf.name_scope('Discriminator_Loss_Real'): #The Discriminator will set Real Data input as 1. 1-smooth is used for label smoothning.\n",
    "    dtcost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_tlog, labels= tf.ones_like(d_t)*(1-smooth)))\n",
    "    dtcosts=tf.summary.scalar(\"Discriminator_Loss_Real\",dtcost)\n",
    "\n",
    "with tf.name_scope('Discriminator_Loss'): #The Two losses are added\n",
    "    dcost=dtcost+dfcost\n",
    "    dcosts=tf.summary.scalar('Discriminator_Loss',dcost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('Generator_Loss'): #The Generator will try to fool the Discriminator \n",
    "    gcost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_flog, labels=tf.ones_like(d_f)))\n",
    "    gcosts=tf.summary.scalar('Generator_Loss',gcost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T_vars = tf.trainable_variables() #Making list of Trainable Variables\n",
    "D_vars = [var for var in T_vars if var.name.startswith('discriminator')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('generator')]\n",
    "\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)): #Using Adam Optimizer\n",
    "    D_optim = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(dcost, var_list=D_vars)\n",
    "    G_optim = tf.train.AdamOptimizer(lr, beta1=0.5).minimize(gcost, var_list=G_vars)\n",
    "#Tensorboard Operation\n",
    "d_summary=tf.summary.merge([dcosts])\n",
    "g_summary=tf.summary.merge([gcosts,gs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver(var_list=G_vars)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter(logs_path)\n",
    "    summary_writer.add_graph(graph=sess.graph)        \n",
    "    gstep=1\n",
    "    for e in range(epochs):\n",
    "        g_loss = []\n",
    "        d_loss = [] \n",
    "        for ii in range(mnist.train.num_examples//batch_size):\n",
    "            batch = mnist.train.next_batch(batch_size)\n",
    "            batch_images = batch[0].reshape((batch_size, 784))\n",
    "            batch_images = batch_images*2 - 1\n",
    "            batch_z = np.random.uniform(-1, 1, size=(batch_size, 100))\n",
    "            _,dl,summary1 = sess.run([D_optim,dcost,d_summary] ,feed_dict={x: batch_images, z_dim: batch_z})\n",
    "            _ ,gl,summary2= sess.run([G_optim,gcost,g_summary], feed_dict={z_dim: batch_z})\n",
    "            summary_writer.add_summary(summary1,gstep)\n",
    "            summary_writer.add_summary(summary2,gstep)\n",
    "            gstep+=1\n",
    "            d_loss.append([dl])\n",
    "            g_loss.append([gl])\n",
    "        fixed_p = root + 'Fixed_results/' + model + str(e + 1) + '.png'\n",
    "        show_result((e + 1), save=True, path=fixed_p)\n",
    "        print('Epoch', e, 'completed out of',epochs,'generator loss:',np.mean(g_loss),'discriminator loss:',np.mean(d_loss))\n",
    "        saver.save(sess, './checkpoints/generator.ckpt')\n",
    "    images = []\n",
    "    for e in range(epochs):\n",
    "        img_name = root + 'Fixed_results/' + model + str(e + 1) + '.png'\n",
    "        images.append(imageio.imread(img_name))\n",
    "    imageio.mimsave(root + model + 'generation_animation.gif', images, fps=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Generated Samples \n",
    "<img src=\"images/MNIST_GAN_generation_animation.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run the command line:\n",
      "--> tensorboard --logdir=vgan \n",
      "Then open http://0.0.0.0:6006/ into your web browser\n"
     ]
    }
   ],
   "source": [
    "print(\"Run the command line:\\n\" \\\n",
    "          \"--> tensorboard --logdir=vgan \" \\\n",
    "          \"\\nThen open http://0.0.0.0:6006/ into your web browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Graph Of the Network\n",
    "<img src=\"images/graph.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Discriminator and Generator Losses\n",
    "<img src=\"images/lossgd.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
